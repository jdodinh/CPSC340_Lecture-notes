% ---
\documentclass{article}


% Packages
% ---
\usepackage{amsmath,amssymb,amsthm} 	% Advanced math typesetting
% \usepackage[utf8]{inputenc} 	% Unicode support (Umlauts etc.)
\usepackage[USenglish]{babel} 	% Change hyphenation rules
\usepackage{hyperref} 				% Add a link to your document
\usepackage{graphicx}				% Add pictures to your document
\graphicspath{ {./images/} }	% image directory

% Typewriter settings for code
\usepackage{listings} 				% Source code formatting and highlighting
\lstset{basicstyle=\ttfamily}		%Typewriter font for code writing


\usepackage{enumitem}

% Margins
\usepackage{geometry}
\geometry{margin=1in}


\usepackage{float}
\restylefloat{figure}
\usepackage{mathabx}
\usepackage{fancyhdr}
\usepackage[dvipsnames]{xcolor}
\usepackage{scrextend}

% Siderules for digressions/examples
\usepackage{mdframed}
\newmdenv[
topline=false,
bottomline=false,
skipabove=\topsep,
skipbelow=\topsep
]{siderules}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}


% Headers
\pagestyle{fancy}
\fancyhf{}
\lhead{\bf CPSC 340 \\ Week 6 }
\rhead{\bf Jeremi Do Dinh \\ 61985628}
\rfoot{Page \thepage}

% Graph drawing tools
\usepackage{tikz}						
\usetikzlibrary {positioning}

\usepackage{breqn}
\usepackage{multicol} 				% Multiple column functionality
\usepackage{blindtext}

% Figure shortcut
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}


\begin{document}

\noindent \url{https://www.cs.ubc.ca/~fwood/CS340/}

\section*{Lecture XVI - Feature selection}
\textbf{February 10th, 2020} \\
\url{https://www.cs.ubc.ca/~fwood/CS340/lectures/L16.pdf}

\subsection*{Feature Selection}
To discuss feature selection we recall the motivating example for decision tree: The food allergy example. 

\centerfig{0.75}{food-allergies}
Here instead of predicting "sick" we want to do feature selection. This means that we want to discover \blu{which foods are "relevant" in predicting \textit{sick}}. \\ \\
The general feature selection problem is outlined as follows:
\centerfig{0.50}{feature-sel1}
\textbf{\gre{Find the features (colkumns) of $ X $ that are important in predicting $ y $}}
\begin{itemize}
	\item “What are the relevant factors?”
	\item “Which basis functions should I use among these choices?”
	\item “What types of new data should I collect?”
	\item “How can I speed up computation?”
\end{itemize}
This is one of the most important problems in ML/statistics, but it is very messy:
\begin{itemize}
	\item For now, we’ll say a feature is “relevant” if it helps \blu{predict $ y_i $ from $ x_i $}.
\end{itemize}

\subsection*{"Association" Approach}
A simple/common way to do feature selection is the following:
\begin{itemize}
	\item For each feature ‘j’, compute correlation between feature values $ x_j $ and $ y $. We say that $ j $ is relevant if correlation is above $ 0.9 $ or below $ -0.9 $.
	\item This turns feature selection into hypothesis testing for each feature.
\end{itemize}
However this usually gives unsatisfactory results as it \red{ignores variable interactions}: 
\begin{itemize}
	\item \red{Includes irrelevant variables}: “Taco Tuesdays”.
\begin{itemize}
	\item If tacos make you sick, and you often eat tacos on Tuesdays, it will say “Tuesday” is relevant. 
\end{itemize}
\item \red{Excludes relevant variables}: “Diet Coke + Mentos Eruption”.
\begin{itemize}
	\item Diet coke and Mentos don’t make you sick on their own, but \textit{together} they make you sick.
\end{itemize}
\end{itemize}

\subsection*{"Regression Weight" Approach}
This is a simple and common approach to feature selection. We :
\begin{itemize}
	\item \blu{Fit regression weights $ w $ based on all features }(maybe with least squares).
	\item Take all features $ j $ where \blu{weight $ |w_j| $ is greater than a threshold.}
\end{itemize}
For example if you fit a least squares model with 5 features and get:
\begin{align*}
w &= \begin{bmatrix}
0.01 \\ -0.2 \\ 10 \\ -3 \\ 0.0001
\end{bmatrix}
\end{align*}
We observe that:
\begin{itemize}[label = -]
	\item Feature 3 looks the most relevant. 
	\item Feature 4 also looks relevant.
	\item Feature 5 seems irrelevant.
\end{itemize}
This could:
\begin{itemize}
	\item Recognize that “Tuesday” is irrelevant.
	\begin{itemize}
		\item If you get enough data, and you sometimes eat tacos on other days. (And the relationship is actually linear.)
	\end{itemize}
	\item Recognize that “Diet Coke” and “Mentos” are relevant. 
	\begin{itemize}
		\item Assuming this combination occurs enough times in the data.
	\end{itemize}
\end{itemize}
\textbf{However:} this problem has \red{major problems with collinearity}:
\begin{itemize}
	\item If the “Tuesday” variable always equals the “taco” variable, it \red{could say that Tuesdays are relevant but tacos are not.} 
	\centerfig{0.75}{reg-weight-1}
	\item If you have two copies of an irrelevant feature, \red{it could take both irrelevant copies}.
		\centerfig{0.9}{reg-weight-2}
\end{itemize}

\subsection*{Search and Score Methods}
The most common feature selection method is \blu{search and score}. It focuses around:
\begin{enumerate}
	\item Define \blu{score function $ f(S) $} that measures quality of a set of features $ S $.
	\item Now \blu{search} for the variables $ S $ with the best score
\end{enumerate}
Example with 3 features:
\begin{itemize} [label = -]
	
	\item Compute “score” of using feature $ 1 $. 
	\item Compute “score” of using feature $ 2 $. 
	\item Compute “score” of using feature $ 3 $. 
	\item Compute “score” of using features $ \{1,2\} $. 
	\item Compute “score” of using features $ \{2,3\} $.
	\item Compute “score” of using features $ \{1,3\} $.
	\item Compute “score” of using features $ \{1,2, 3\} $.
	\item Compute “score” of using feature $ \{ \} $
	\item Return the set of features $ S $ with the best “score”.
\end{itemize}

\subsubsection*{Which Score function?}
Firstly, the \red{score can’t be the training error}:
\begin{itemize}[label = -]
	\item because training error goes down as you add features, so will \red{select all features}.
\end{itemize}
Therefore a more logical score function would be the \blu{validation error}. 
\begin{itemize}[label = -]
	\item “\gre{Find the set of features that gives the lowest validation error.}” 
	\item To minimize test error, this is what we want.
\end{itemize}
However there are problems due to the large \red{number of sets of variables}:
\begin{itemize}[label = -]
	\item If we have $ d $ variables, there are \red{$ 2^d $ sets} of variables.
	\item \red{Optimization bias} is high: we’re optimizing over $ 2^d $ models (not 10).
	\item Prone to \red{false positives}: irrelevant variables will sometimes help by chance.
\end{itemize}

\subsubsection*{Number of Features” Penalties}
To reduce false positives, we can again use complexity penalties:
\centerfig{0.75}{comp-pen-1}
\begin{itemize}[label = -]
	\item E.g., we could use squared error and number of non-zeroes. 
	\item We’re using $ x_{iS} $ as the features $ S $ of example $ x_i $.
\end{itemize}
Here we have that if two $ S $ have similar error, this prefers the smaller set:
\begin{itemize}[label=-]
	\item It prefers removing feature 3 instead of having $ w_3 = 0.00001 $.
\end{itemize}
Instead of “size(S)”, we usually write this using the “L0-norm”

\subsubsection*{L0-Norm and “Number of Features We Use”}
In linear models, \red{setting $ w_j = 0 $ is the same as removing feature $ j $}:
\centerfig{0.65}{L0-norm-1}
The L0-Norm is the number of non-zero values ($ \norm{w}_0 = \text{size}(S) $)


\subsubsection*{L0-penalty: optimization}
L0-norm penalty for feature selection is:
\centerfig{0.5}{L0-norm-2}
Suppose we want to use this to evaluate the features $ S=\{1,2\} $: 
\begin{itemize}
	\item First fit the $ w $ just using features 1 and 2.
	\item Now compute the training error with this $ w $ and features 1 and 2. 
	\item Add $ 2 \lambda $ to the training error to get the score.
\end{itemize}
We repeat this with other choices of $ S $ to find the best features. \\ \\
The above equation balances between training error and number of features we use.:
\begin{itemize}
	\item With $ \lambda=0 $, we get least squares with all features.
	\item With $ \lambda = \inf $, we must set $ w=0 $ and not use any features.
	\item With other $\lambda$, balances between training error and number of non-zeroes.
	\begin{itemize}
		\item Larger $\lambda$ puts more emphasis on having zeroes in $ w $ (more features removed).
		\item Different values give AIC, BIC, and so on.
	\end{itemize}
\end{itemize}
\subsection*{Forward Selection (Greedy Search Heuristic)}
In search and score, it’s also just \red{hard to search for the best $ S $}, since there are \red{$ 2^d $ possible sets}. \\ \\
A common greedy approach is \textbf{\blu{forward selection}}:
\begin{enumerate}
	\item \gre{Compute score if we use \underline{no} features}
	\item \gre{Try adding "$ taco $", "$ milk $", "$ egg $", and so on (computing score for each)}
	\item Add "$ milk $" because it got the best score. 
	\item \gre{Try $ \{milk, taco\} $, $ \{milk, egg\} $ and so on, computing \underline{score} of each variable \underline{with $ milk $}}
	\item Add "$ egg $" because it got the best score combined with "$ milk $"
	\item \gre{Try $ \{milk, egg, taco\} $, $ \{milk, egg, pizza\} $}
\end{enumerate}
Formally, the \blu{forward selection} search algorithm is as follows:
\begin{enumerate}
	\item Start with an \gre{empty set} of features, $ S = [\quad] $.
	\item For each possible feature $ j $
	\begin{itemize}[label=-]
		\item \gre{Compute the scores of features in $ S $ combined with feature $ j $}
	\end{itemize}
	\item Find the $ j $ that has the highest score when added to $ S $.
	\item Check if {$S \cup j$} improves on the best score found so far.
	\item Add $ j $ to $ S $ and go back to Step 2.
	\begin{itemize}[label=-]
		\item A variation is to \red{stop if no $ j $ improves the score} over just using $ S $.
	\end{itemize}
\end{enumerate}
This method is \red{not guaranteed to find the best set}, \gre{but reduces many problems}:
\begin{itemize}
	\item Considers $ O(d^2) $ models: cheaper, overfits less, has fewer false positives
\end{itemize}

\subsection*{In summary}
We set out to choose the relevant features:
\centerfig{0.4}{summ-1}
The most common approach is \blu{search and score}:
\begin{itemize}
	\item Define “score” and “search” for features with best score.
\end{itemize}
But it’s \red{hard to define the “score” and it’s hard to “search”}. 
\begin{itemize}
	\item So we often use greedy methods like \blu{forward selection}.
\end{itemize}
Methods work ok on “toy” data, but are \red{frustrating on real data}. 
\begin{itemize}
	\item Different methods may return very different results.
	\item Defining whether a feature is “relevant” is complicated and ambiguous.
\end{itemize}
The Advice is choosing the relevant variables is:
\begin{itemize}
	\item Try the association approach.
	\item Try forward selection with different values of $\lambda$.
	\item Try out a few other feature selection methods too.
\end{itemize}
\textbf{Then:}
\begin{itemize}
	\item \gre{\textbf{Discuss the results}} with the domain expert.
	\begin{itemize}
		\item They probably have an idea of why some variables might be relevant.
	\end{itemize}
\item \red{\textbf{Don’t be over confident}}
\begin{itemize}
	\item These methods are probably not discovering how the world truly works.
	\item “The algorithm has found that these variables are helpful in predicting yi.” 
	\begin{itemize}
		\item Then a warning that these models are not perfect at finding relevant variables.
	\end{itemize}
\end{itemize}
\end{itemize}
\newpage

\section*{Lecture XVII - Regularization}
\textbf{February 12th, 2020} \\
\url{https://www.cs.ubc.ca/~fwood/CS340/lectures/L17.pdf}

\subsection*{“Feature” Selection vs. “Model” Selection?}
\blu{\textbf{Model selection}}: “which model should I use?”
\begin{itemize}
	\item KNN vs. decision tree, depth of decision tree, \gre{degree of polynomial basis}. 
\end{itemize}
\blu{\textbf{Feature selection}}:“which features should I use?”
\begin{itemize}
	\item Using feature 10 or not, \gre{using $ x_i^2 $ as part of basis}.
\end{itemize}
These two tasks are \gre{\textbf{highly-related}}:
\begin{itemize}
	\item It’s a different “model” if we add $ x_i^2 $ to linear regression.
	\item But the $ x_i^2 $ term is just a “feature” that could be “selected” or not.
	\item Usually, “feature selection” means choosing from some “original” features. 
	\begin{itemize}
		\item You could say that “feature” selection is a special case of “model” selection.
	\end{itemize}
\end{itemize}

\subsubsection*{Can it help prediction to throw features away?}
Yes, because \red{linear regression can over-fit} with large $ d $ (Even though it’s “just” a hyper-plane.) \\ \\
Consider using $ d = n $, with completely random features. 
\begin{itemize}
	\item With high probability, you will be able to \red{get a training error of 0}. 
	\item But the features were random, this is \red{completely overfitting}.
\end{itemize}
You could view \gre{“number of features” as a hyper-parameter}:
\begin{itemize}
	\item Model gets more complex as you add more features.
\end{itemize}


\subsection*{Controlling Complexity}
We’ve said that \red{complicated models tend to overfit more}. But what if we need a complicated model? \\ \\
Usually \red{“true” mapping from $ x_i $ to $ y_i $ is complex.}
\begin{itemize}
	\item Might need high-degree polynomial.
	\item Might need to combine many features, and don’t know “relevant” ones
\end{itemize}
But complex models can overfit, so what do we do??? \\ \\
Our main tools are:
\begin{enumerate}
	\item \blu{\textbf{Model averaging}}: average over multiple models to decrease variance
	\item \blu{\textbf{Regularization}}: add a \gre{penalty on the complexity} of the model.
\end{enumerate}
\subsubsection*{Consider the following dataset and 3 linear regression models:}
\centerfig{0.5}{comp-pen-2}
Which line should we choose? \\ \\
If we are forced to choose betweed \red{red} and \gre{green} assuming they have the same training error, we \textbf{\gre{should pick green}}
\begin{itemize}
	\item Since slope is smaller, \gre{small change in $ x_i $ has a smaller change in prediction $ y_i $}.
	\begin{itemize}
		\item Green line’s predictions are \gre{less sensitive to having $ w $ exactly right}.
	\end{itemize}
	\item Since green $ w $ is less sensitive to data, test error might be lower.
\end{itemize}
 \subsection*{L2-regularization}
 
\subsubsection*{Motivation}

The standard strategy for \blu{regularization} is \blu{L2-regularization}. 
\centerfig{0.75}{L2-reg-1}
The intuition is that \red{large slopes $ w_j $} tend to lead to overfitting. Consequently the objective \gre{balances getting low error vs. having small slopes} $ w_j $:
\begin{itemize}
	\item “You can increase the training error if it makes ‘w’ much smaller.”
	\item This nearly-always reduces overfitting.
	\item The regularization parameter $\lambda > 0$ controls “strength” of regularization. 
	\begin{itemize}
		\item Large $\lambda$ puts large penalty on slopes.
	\end{itemize}
\end{itemize}
In terms of the fundamental trade-off:
\begin{itemize}
	\item Regularization \red{increases training error}.
	\item Regularization \gre{decreases approximation error}.
\end{itemize}
Therefore now the question is: How do we choose $\lambda$?
\begin{itemize}
	\item Theory: as $ n $ grows $\lambda$ should be in the range $ O(1) $ to $ (\sqrt{n}) $.
	\item Practice: optimize \blu{validation set} or \blu{cross-validation error}
	\begin{itemize}
		\item \gre{This almost always decreases the test error.}
	\end{itemize}
\end{itemize}
\subsubsection*{L2-Regularization “Shrinking” Example}
Solution to a “\blu{least squares with L2-regularization}” for \gre{different $\lambda$}:
\centerfig{0.9}{L2-reg-ex1}
We obtain \gre{least squares when $\lambda = 0$}, but we can achieve similar training error with smaller $ \norm{w} $ \\ \\
$ \norm{Xw –y} $ increases with $\lambda$, and $ \norm{w} $ decreases with $\lambda$.
\begin{itemize}
	\item Though individual $ w_j $ can increase or decrease with lambda.
	\item Because we use the L2-norm, the large ones decrease the most.
\end{itemize}

\subsubsection*{Regularization Path}
Regularization path is a plot of the optimal weights $ w_j $ as $\lambda$ varies.
\centerfig{0.5}{Reg-path}
Starts with least squares with $\lambda= 0$, and $ w_j $ converge to 0 as $\lambda$ grows.

\subsection*{L2-regularization and the normal equations}

When using the L2-regularized squared error, we can solve for $ \nabla f(w) = 0 $. We have that:
\begin{itemize}
	\item Loss before: $ f(w) = \frac{1}{2} \norm{Xw-y}^2 $
	\item Loss after: $ f(w) = \frac{1}{2} \norm{Xw-y}^2 + \frac{\lambda}{2} \norm{w}^2$
\end{itemize}
We also have that:
\begin{itemize}
	\item Gradient before: $ \nabla f(w) = X^T X w - X^Ty $
	\item Gradient after: $ \nabla f(w) = X^T X w - X^Ty + \lambda w$
\end{itemize}
And:
\begin{itemize}
	\item Linear system before: $ X^TXw = X^Ty $
	\item Linear system after: $ (X^TX + \lambda I )w = X^Ty $
\end{itemize}
But unlike $ X^TX $, the matrix $ (X^TX+ \lambda I) $ is always invertible. Then we multiply by its inverse to get $ w = (X^TX + \lambda I )^{-1} (X^Ty) $

\subsection*{Gradient Descent for L2-Regularized Least Squares}
The L2-regularized least squares objective and gradient is:
\begin{align*}
f(w) &= \frac{1}{2} \norm{Xw - y} ^ 2 + \gre{\frac{\lambda}{2} \norm{w} ^ 2} \\
\nabla f(w) &= X^T (Xw - y) \gre{+ \lambda w}
\end{align*}
The gradient descent iterations for L2-regularized least squares is:
\begin{align*}
w ^ {t+1} &= w^t - \alpha ^ t (\underbrace{X^T (Xw^t - y) \gre{+ \lambda w^t}}_{\nabla f(w^t)})
\end{align*}
Here the cost of a gradient descent iteration is still $ O(nd) $. We can also show that the \gre{number of iterations decreases as $ \lambda $ increases} (which is not an obvious proof).

\begin{siderules}
	\textbf{Digression: Why use L2-Regularization?} \\ \\
	In general it "Almost always decreases the test error". However here are 6 more reasons:
	\begin{enumerate}
		\item Solution to $ w $ is \gre{unique}.
		\item $ X^TX $ \gre{does not have to be invertible}
		\item \gre{Less sensitive} in changes to $ X $ or $ y $. 
		\item Gradient descent \gre{converges faster} (bigger $\lambda$ means fewer iterations).
		\item \textsl{Stein's paradox}: if $ d \geq 3 $, 'shrinking' \gre{moves us closer to 'true' $ w $}.
		\item Worst case: just set $\lambda$ small and get the same performance.
	\end{enumerate}
\end{siderules}

\subsection*{Standardizing Features}
\subsubsection*{Features with Different Scales}
Let's consider continuous features with different scales:
\centerfig{0.6}{diff-scales-1}
Should we convert them to a certain normalized/standardized unit?
\begin{itemize}
	\item It \gre{doesn't matter for decision trees or Naïve Bayes}, since they only look at one feature at a time.
	\item It also \gre{doesn't matter for Least squares}, since unit conversion doesn't affect the model. 
\end{itemize}
\textbf{\textit{\underline{However}:}}
\begin{itemize}
	\item It \red{matters for K-Nearest Neighbors}, since distance will be affected more by large features than small features
	\item It \red{matters for regularized least squares}, since penalizing $ w_j^2 $ means different things if features $ j $ are on different scales.
\end{itemize}
Consequently it is common to \blu{standardize continuous features}:
\begin{itemize}
	\item \texttt{For each feature}:
	\begin{itemize}
		\item \texttt{Compute the mean and standard deviation}:
		\centerfig{0.5}{stand-1}
		\item \texttt{Subtract mean and divide by standard deviation (“z-score”)}
				\centerfig{0.5}{stand-2}
	\end{itemize}
\item Now changes in in $ w_j $ have the same effect for any feature $ j $
\end{itemize}
\textbf{How should we standardize test data?}
\begin{itemize}
	\item \red{The wrong approach is} to use the mean and standard deviation of the test data
	\item The training and test mean and standard deviation might be very different. 
	\item The \gre{right approach is to use the mean and standard deviation of the training data}
\end{itemize}
\textbf{If we're doing 10-fold cross validation:}
\begin{itemize}
	\item Compute $ \mu_j $ and $ \sigma_j $ based on the 9 training folds (e.g.: average over 9/10s of the data)
	\item Standardize the remaining "validation" fold with this training $ \mu_j $ and $ \sigma_j $
	\item Re-standardize for the different folds
\end{itemize}

\subsubsection*{Standardizing Target}
In regression we sometimes \gre{standardize the targets $ y_i $.} This puts the targets on the same standard scale as the standardized features:
\centerfig{0.5}{stand-3}
With a standardized target, setting $ w = 0 $ \gre{predicts average $ y_i $}, which means that higher regularization lets us predict closer to the average value. \\
\\
But it is important to remember to \red{\textbf{standardize test data with the training sets!}} \\
\\
Other common approaches to transformations are the use of exponents or logarithms:
\centerfig{0.5}{stand-4}

\newpage

\section*{Lecture XVIII - More Regularization}
\textbf{February 14th, 2020} \\
\url{https://www.cs.ubc.ca/~fwood/CS340/lectures/L18.pdf}

\subsection*{ Parametric vs. Non-Parametric Transforms}
So far we've been using linear models with \blu{polynomial bases}
\centerfig{0.80}{param-1}
But polynomials are not the only possible bases:
\begin{itemize}
	\item Exponentials, logarithmis, trigonometric functions, etc
	\item The \gre{right basis will vastly improve performance}
	\item If we use the wrong basis, the accuracy will be limited even with a large amount of data
	\item But the \red{right bases might not be obvious!}
\end{itemize}
An alternative is \textbf{non-parametric bases:}
\begin{itemize}
	\item The size of the basis (number of features) \gre{grown with $ n $}
	\item Model gets more complicated as you get more data
	\item It can model complicated functions where you don't know the right basis (\textsl{with enough data})
	\item A classic example is \textit{\textbf{\blu{Gaussian RBF's}}} ("Gaussian" = "Normal Distribution")
\end{itemize}

\subsection*{Gaussian RBF's: A sum of "bumps"}
\centerfig{0.9}{gauss-1}
Gaussian RBFs are \blu{universal approximators} (compact subsets of $ \mathbb
{R}^d $):
\begin{itemize}
	\item Enough bumps can \gre{approximate any continuous function} to an arbitrary precision
	\item This means we can \gre{achieve optimal test error} as $ n $ goes to infinity
\end{itemize}
\textbf{Comparison}:
\begin{itemize}
	\item \textit{Polynomial Fit:}
	\centerfig{0.8}{poly-fit-1}
	\item \textit{Constructing a function from bumps ("smooth histogram")}
	\centerfig{0.8}{gauss-fit-1}
\end{itemize}

\subsubsection*{Gaussian RBFs parameters}
Some obvious \textbf{questions} are:
\begin{enumerate}
	\item How many bumps should we use?
	\item Should the bumps be centered?
	\item How far up should the bumps go?
	\item How wide should the bumps be?
\end{enumerate}
The usual \textbf{answers} are:
\begin{enumerate}
	\item We \gre{use $ n $ bumps} (non-parametric basis)
	\item Each bump is \gre{centered on one training example $ x_i $}
	\item Fitting regression weights \gre{$ w $ gives us the heights} (and signs)
	\item The \gre{width is a hyper parameter} (narrow bumps = complicated model).
\end{enumerate}

\subsubsection*{Gaussian RBFs: Formal Details}
What is a \blu{\textbf{radial basis functions}} (RBFs)?
\begin{itemize}[label=-]
	\item A set of non-parametric bases that \gre{depend on distances to training points}
	\centerfig{1}{gauss-2}
	\item We have $ n $ features with \gre{feature $ j $ depending on its distance to example $ i $}
	\item The most common $ g $ is the Gaussian RBF:
	\begin{align*}
	g(\varepsilon) &= \exp (- \frac{\varepsilon^2}{2 \sigma ^2})
	\end{align*}
	\begin{itemize}
		\item The variance $ \sigma^2 $ is a hyper-parameter controlling the "width". This affects fundamental trade-off (and we set it using a validation set).
	\end{itemize}
\centerfig{0.3}{gauss-3}
\end{itemize} 
\centerfig{1}{gauss-4}

\subsubsection*{Pseudocode}
Here is the pseudo code for construction RBFs given data $ X $ and a hyper-parameter $ \sigma $
\begin{lstlisting}[tabsize=3]
	Z = zcros(n,n)
	for i1 in 1:n :
		for i2 in 1:n :
			Z[i1, i2] = exp(-norm(X[i1,:] - X[i2,:])^2 / (2 * sig^2))
\end{lstlisting}
With the test data $ \tilde{X}$: form $ \tilde{Z} $ based on distances to training examples. \\
\\
And if we wish to do least squares for different values of the hyper-parameter $ \sigma $, then we could add a bias, and a linear basis:
\centerfig{0.6}{bias-lin-basis}
This reverts to linear regression, instead of 0 away from the data.

\subsection*{ RBFs, Regularization and Validation}
We make predictions using rbf's as follows:
\begin{align*}
\hat{y}_i &= w_1 \exp\left(- \frac{\norm{x_i - x_1}^2}{2 \sigma ^2}\right) + w_2 \exp\left(- \frac{\norm{x_i - x_2}^2}{2 \sigma ^2}\right) + \dots + w_n \exp\left(- \frac{\norm{x_i - x_n}^2}{2 \sigma ^2}\right) \\
&= \sum_{j = 1}^{n} w_j \exp\left(- \frac{\norm{x_i - x_j}^2}{2 \sigma ^2}\right)
\end{align*}
Here:
\begin{itemize}
	\item The flexible basis can \gre{model any continuous function}.
	\item But with $ n $ data points RBFs \red{have $ n $ basis functions}.
\end{itemize}
How can we avoid over-fitting with this huge number of features?\\
\\
\textbf{We \blu{regularize} $ w $ and use \blu{validation error} to choose \gre{$ \sigma $ and $ \lambda $}} \\ \\
This turns out to be a model that is hard to beat:
\begin{itemize}
	\item \gre{RBF basis with L2-regularization and cross validation to choose $ \sigma $ and $ \lambda $}
	\item Flexible non-parametric basis, magic of regularization, and tuning for test error.
\end{itemize}
\begin{lstlisting}[tabsize=3]
	for each value of sig and lamd:
		Compute Z on training data (and sig)
		Compute best v: v = (Z.T@Z + lamd*I)^(-1) @ Z.T@y
		Compute Z_tilde on validation data (using train data distances)
\end{lstlisting}


\end{document}